{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Introduction\n",
        "Fine tune an LLM using Huggingface platform"
      ],
      "metadata": {
        "id": "3ebtz2EJMJfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install\n",
        "_Run once_"
      ],
      "metadata": {
        "id": "nXcF3J4ZMrO_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "5Lpx1tn7C0uT"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade pip\n",
        "\n",
        "# !pip install unsloth\n",
        "# !pip install \"torch >= 2.0.0\"\n",
        "# !pip install \"bitsandbytes >= 0.40.0\"\n",
        "# !pip install \"transformers >= 4.30.0\"\n",
        "# !pip install \"accelerate >= 0.20.3\"\n",
        "# !pip install \"datasets >= 2.12.0\"\n",
        "# !pip install \"trl > 0.6.0\"\n",
        "# !pip install \"unsloth >= 0.1.0\"\n",
        "# !pip install \"peft >= 0.4.0\"\n",
        "# !pip install \"evaluate\"\n",
        "# !pip install \"tensorboard\"\n",
        "\n",
        "# !pip install colorama"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Imports"
      ],
      "metadata": {
        "id": "8eRSk6eKNzUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "import accelerate\n",
        "\n",
        "import trl\n",
        "\n",
        "import datasets as ds\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "from datasets import DatasetDict\n",
        "from datasets import IterableDataset\n",
        "from datasets import IterableDatasetDict\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "from typing import Any\n",
        "\n",
        "from colorama import Fore, Back, Style"
      ],
      "metadata": {
        "id": "CDo50Oy9Nx5x"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Check versions"
      ],
      "metadata": {
        "id": "6DUXsgv90R1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Unsloth Version\\t\\t: {Style.BRIGHT}{Fore.CYAN}{unsloth.__version__}{Style.RESET_ALL}\")\n",
        "print(f\"Torch Version\\t\\t: {Style.BRIGHT}{Fore.CYAN}{torch.__version__}{Style.RESET_ALL}\")\n",
        "print(f\"BitsAndBytes Version\\t: {Style.BRIGHT}{Fore.CYAN}{bnb.__version__}{Style.RESET_ALL}\")\n",
        "print(f\"Transformers Version\\t: {Style.BRIGHT}{Fore.CYAN}{transformers.__version__}{Style.RESET_ALL}\")\n",
        "print(f\"Accelerate Version\\t: {Style.BRIGHT}{Fore.CYAN}{accelerate.__version__}{Style.RESET_ALL}\")\n",
        "print(f\"Datasets Version\\t: {Style.BRIGHT}{Fore.CYAN}{ds.__version__}{Style.RESET_ALL}\")\n",
        "print(f\"TRL Version\\t\\t: {Style.BRIGHT}{Fore.CYAN}{trl.__version__}{Style.RESET_ALL}\")\n",
        "print(f\"\\nCUDA Availability\\t: {Style.BRIGHT}{Fore.GREEN}{torch.cuda.is_available()}{Style.RESET_ALL}\")\n",
        "print(f\"CUDA Device Count\\t: {Style.BRIGHT}{Fore.GREEN}{torch.cuda.device_count()}{Style.RESET_ALL}\")"
      ],
      "metadata": {
        "id": "SwCQKhwXzyAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed546917-b5e8-47ad-885d-e5612e2da843"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth Version\t\t: \u001b[1m\u001b[36m2025.6.2\u001b[0m\n",
            "Torch Version\t\t: \u001b[1m\u001b[36m2.7.0+cu126\u001b[0m\n",
            "BitsAndBytes Version\t: \u001b[1m\u001b[36m0.46.0\u001b[0m\n",
            "Transformers Version\t: \u001b[1m\u001b[36m4.52.4\u001b[0m\n",
            "Accelerate Version\t: \u001b[1m\u001b[36m1.7.0\u001b[0m\n",
            "Datasets Version\t: \u001b[1m\u001b[36m3.6.0\u001b[0m\n",
            "TRL Version\t\t: \u001b[1m\u001b[36m0.18.1\u001b[0m\n",
            "\n",
            "CUDA Availability\t: \u001b[1m\u001b[32mTrue\u001b[0m\n",
            "CUDA Device Count\t: \u001b[1m\u001b[32m1\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Variables"
      ],
      "metadata": {
        "id": "eJjP8Zpn3Wd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Environment Variables"
      ],
      "metadata": {
        "id": "_H9NeYfH3Z_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configured env var for Unsloth <---- particularly for colab notebook\n",
        "os.environ[\"UNSLOTH_COMPILE_DISABLE\"] = \"1\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "metadata": {
        "id": "UL3K_ytM1pGB"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. User Variables"
      ],
      "metadata": {
        "id": "nYjlngg2_bla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps: int = 1000\n",
        "\n",
        "model_name: str = \"distilgpt2\"\n",
        "dataset_name: str = \"timdettmers/openassistant-guanaco\"\n",
        "gigs: int = 1024**3"
      ],
      "metadata": {
        "id": "JFmtxYk4_h7R"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Code"
      ],
      "metadata": {
        "id": "8YnojQ-Y_T1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Prepare dataset\n",
        "Steps:\n",
        "<ol>\n",
        "    <li>Fetch a dataset</li>\n",
        "    <li>Put an <font color=\"#22819F\"><b>eos_token</b></font> if the examples do not have one at the end</li>\n",
        "    <li>For tokenized dataset, change token ids to a reserved keyword (<i>-100 here</i>) for pad tokens</li>\n",
        "    <li>Tokenize</li>\n",
        "</ol>"
      ],
      "metadata": {
        "id": "uTfEComlSKtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.1. Fetch a dataset\n",
        "(from Hugging face)"
      ],
      "metadata": {
        "id": "djAVmSVxWCld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(dataset_name: str, max_samples: int\n",
        "    ) -> Dataset | DatasetDict | IterableDataset | IterableDatasetDict:\n",
        "    \"\"\"\n",
        "    Get dataset from HuggingFace.\n",
        "\n",
        "    Args:\n",
        "        dataset_name (str): Name of the dataset.\n",
        "        max_samples (int): Maximum number of samples in the dataset to use.\n",
        "\n",
        "    Returns:\n",
        "        Dataset | DatasetDict | IterableDataset | IterableDatasetDict\n",
        "    \"\"\"\n",
        "\n",
        "    dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "    if max_samples is not None and max_samples < len(dataset):\n",
        "        dataset = dataset.select(range(max_samples))\n",
        "\n",
        "    print(f\"Dataset {Fore.LIGHTBLUE_EX}{dataset_name}{Style.RESET_ALL} loaded successfully.\")\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "dH9Oio1FWL6Z"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenizer(model_name: str) -> Any:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    print(f\"Tokenizer {Fore.LIGHTBLUE_EX}{model_name}{Style.RESET_ALL} loaded successfully.\")\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "OSL_qjIGWdAA"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.2. Put an <font color=\"#22819F\"><b>eos_token</b></font> if the examples do not have one at the end"
      ],
      "metadata": {
        "id": "RFPqnjvOWQKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_pad_tokens_at_end(texts, tokenizer) -> list[str]:\n",
        "    processed_texts: list[str] = []\n",
        "\n",
        "    for text in texts:\n",
        "        if not text.endswith(tokenizer.eos_token):\n",
        "            text += tokenizer.eos_token\n",
        "\n",
        "        processed_texts.append(text)\n",
        "\n",
        "    return processed_texts"
      ],
      "metadata": {
        "id": "i0u8SqDd2Hb_"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.3. Change <font color=\"#22819F\">pad_token id</font> to reserved keyword"
      ],
      "metadata": {
        "id": "5BWYSusw2VwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def change_pad_tokens_ids_to_res_keyword(tokenized_text_id: list[int], tokenizer: Any) -> list[int]:\n",
        "    tokenized_texts_label_ids = []\n",
        "\n",
        "    for label_token_ids in tokenized_text_id:\n",
        "        processed_label_token_ids: list[str] = []\n",
        "\n",
        "        for token_id in label_token_ids:\n",
        "            if token_id == tokenizer.pad_token_id:\n",
        "                token_id = -100\n",
        "\n",
        "            processed_label_token_ids.append(token_id)\n",
        "\n",
        "        tokenized_texts_label_ids.append(processed_label_token_ids)\n",
        "\n",
        "    return tokenized_texts_label_ids"
      ],
      "metadata": {
        "id": "nntVOmHE2dpV"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.4. Tokenize"
      ],
      "metadata": {
        "id": "jRdbR9jK18zD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(examples: Dataset | DatasetDict | IterableDataset | IterableDatasetDict,\n",
        "             tokenizer: Any, max_length: int):\n",
        "    texts: list[str] = examples[\"text\"]\n",
        "    processed_texts: list[str] = add_pad_tokens_at_end(texts, tokenizer)\n",
        "\n",
        "    tokenized_texts = tokenizer(\n",
        "        processed_texts,\n",
        "        max_length = max_length,\n",
        "        truncation = True,\n",
        "        padding = \"max_length\",\n",
        "        return_tensors = \"pt\")\n",
        "\n",
        "    tokenized_text_input_id: list[int] = tokenized_texts[\"input_ids\"].clone()\n",
        "    tokenized_texts[\"labels\"] = change_pad_tokens_ids_to_res_keyword(tokenized_text_input_id, tokenizer)\n",
        "\n",
        "    return tokenized_texts"
      ],
      "metadata": {
        "id": "LgOOVs2p_-HL"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset_from_hugging_face(\n",
        "    dataset_name: str = dataset_name,\n",
        "    model_name: str = model_name,\n",
        "    max_samples: int = None,\n",
        "    max_length: int = 512\n",
        "    ) -> tuple[Dataset, Any]:\n",
        "    \"\"\"\n",
        "    Prepare dataset from HuggingFace.\n",
        "\n",
        "    Args:\n",
        "        dataset_name (str): Name of the dataset.\n",
        "        model_name (str): Name of the model.\n",
        "        max_samples (int): Maximum number of samples in the dataset to use.\n",
        "        max_length (int): Maximum length of the input.\n",
        "\n",
        "    Returns:\n",
        "        tuple[ds.Dataset, AutoTokenizer]: Prepared dataset and tokenizer.\n",
        "    \"\"\"\n",
        "    dataset = get_dataset(dataset_name, max_samples)\n",
        "\n",
        "    tokenizer = get_tokenizer(model_name)\n",
        "\n",
        "    tokenize_function_args_map: dict = {\n",
        "        \"tokenizer\": tokenizer,\n",
        "        \"max_length\": max_length\n",
        "        }\n",
        "\n",
        "    print(f\"{Fore.CYAN}Tokenization started{Style.RESET_ALL}\")\n",
        "    tokenized_dataset = dataset.map(tokenize,\n",
        "        fn_kwargs = tokenize_function_args_map,\n",
        "        remove_columns = dataset.column_names,\n",
        "        batched=True, desc = \"Tokenizing dataset\")\n",
        "    print(f\"{Fore.CYAN}Tokenization completed{Style.RESET_ALL}\")\n",
        "\n",
        "    return tokenized_dataset, tokenizer"
      ],
      "metadata": {
        "id": "XihRfRy-1vz-"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.5. Test Data preparation module"
      ],
      "metadata": {
        "id": "9YEr_r-kFddn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_dataset, tokenizer = prepare_dataset_from_hugging_face()"
      ],
      "metadata": {
        "id": "98zHD9hqCC-p"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_dataset"
      ],
      "metadata": {
        "id": "TWGbtFHgxsLc"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer"
      ],
      "metadata": {
        "id": "U1fLonSz8lfF"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. Train with <font color = \"#EE0099\"><b>Lo</b></font>w <font color = \"#EE0099\"><b>R</b></font>ank <font color = \"#EE0099\"><b>A</b></font>daptation"
      ],
      "metadata": {
        "id": "UEU8237sFmPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2.1. Prepare <font color = \"#FF0077\">LoRA</font> Config\n",
        "<i>Note: This config class is a carrier of all values default and user assigned. It is not LoRA constructor in any way.</i>"
      ],
      "metadata": {
        "id": "QFIZKI41H0YN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "import math\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ProductionLoraConfig:\n",
        "    model_name: str = \"openai-community/gpt2\"\n",
        "    max_seq_length: int = 512\n",
        "    dtype: torch.dtype = torch.float16\n",
        "    lora_r: int = 16 # lora attention dimension, or rank of matrices: the minimum number of independent rows / columns in a matrix is rank.\n",
        "    lora_alpha: int = 16 # alpha parameter: factor which is multiplied to LoRA module matrices before it is added to original params. lower values makes LoRA params less significant.\n",
        "    lora_dropout: float = 0.05 # probability for dropping out the LoRA elements in low rank matrices to prevent overfitting. it is regularisation technique.\n",
        "    target_modules: list = None # modules are q_proj, K_proj, etc depending on the architecture\n",
        "    load_in_4bit: bool = True\n",
        "    bnb_4bit_compute_dtype: torch.dtype = torch.float16\n",
        "    bnb_4bit_quant_type: str = \"nf4\"\n",
        "    bnb_4bit_use_double_quant: bool = True\n",
        "    bnb_8bit_quant_type: str = \"nf4\"\n",
        "    bnb_8bit_use_double_quant: bool = True\n",
        "    bnb_8bit_compute_dtype: torch.dtype = torch.float16\n",
        "    max_steps: int = 2000 # risk of overfitting\n",
        "    per_device_train_batch_size: int = 2\n",
        "    gradient_accumulation_steps: int = 4\n",
        "    learning_rate: float = 2e-4\n",
        "    weight_decay: float = 0.001\n",
        "    warmup_steps: int = 100\n",
        "    warmup_ratio: int = 0.1\n",
        "    optimizer: str = \"adamw_8bith\"\n",
        "    save_steps: int = 250\n",
        "    save_total_limit: int = 3\n",
        "    eval_strategy: str = \"steps\"\n",
        "    eval_steps: int = 250\n",
        "    load_best_model_at_end: bool = True\n",
        "    metric_for_best_model: str = \"loss\"\n",
        "    greater_is_better: bool = False # for \"accuracy\" metric, make it true\n",
        "    gradient_checkpointing: bool = True # recommended for large models on a limited GPU memory. In Hugging Face platform, it is implemented as model.gradient_checkpointing_enabled().\n",
        "    dataloader_num_workers: int = 4\n",
        "    remove_unused_columns: bool = False\n",
        "    group_by_length: bool = True\n",
        "    ddp_find_unused_parameters: bool = False\n",
        "    logging_steps: int = 10\n",
        "    report_to: str = \"tensorboard\" # \"wandb\" or \"tensorboard\" or \"none\"\n",
        "    # task_type is ignored here as a param as we choose the type of LLM later on.\n",
        "\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.target_modules is None:\n",
        "            if \"openai-community/gpt2\" in self.model_name:\n",
        "                self.target_modules = [\"c_attn\", \"c_proj\"]\n",
        "            else:\n",
        "                self.target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "\n",
        "\n",
        "    def get_effective_batch_size(self) -> tuple[int, int]:\n",
        "        num_gpus: int = 1\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            num_gpus = torch.cuda.device_count()\n",
        "\n",
        "        effective_batch_size: int = self.per_device_train_batch_size * num_gpus * self.gradient_accumulation_steps\n",
        "\n",
        "        return effective_batch_size, num_gpus\n",
        "\n",
        "\n",
        "    def find_max_tab_count(self, items: str) -> int:\n",
        "        max_length: int = 0\n",
        "\n",
        "        for item in items:\n",
        "            if item.startswith(\"_\"):\n",
        "                continue\n",
        "\n",
        "            if len(item) > max_length:\n",
        "                max_length = len(item)\n",
        "\n",
        "        return int(math.ceil(max_length / 4))\n",
        "\n",
        "\n",
        "    def print_config(self):\n",
        "        print(\"LoRA Configuration\")\n",
        "\n",
        "        max_tab_count = self.find_max_tab_count(self.__dict__.keys())\n",
        "\n",
        "        for field, value in self.__dict__.items():\n",
        "            if not field.startswith(\"_\"):\n",
        "                tab_count = max_tab_count - int(math.floor(len(field) / 4))\n",
        "                tabs: str = \"\\t\" * tab_count\n",
        "\n",
        "                print(f\"{field}{tabs}: {Style.BRIGHT}{Fore.CYAN}{value}{Style.RESET_ALL}\")\n",
        "\n",
        "        effective_batch_size, num_gpus = self.get_effective_batch_size()\n",
        "\n",
        "        print(f\"\\n{'Effective Batch Size':30s}\\t: {Style.BRIGHT}{Fore.CYAN}{effective_batch_size}{Style.RESET_ALL} across {Style.BRIGHT}{Fore.RED}{num_gpus}{Style.RESET_ALL} GPU(s)\\n\\n\")"
      ],
      "metadata": {
        "id": "8BRy29t480c_"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2.2. Configure LoRA using the above carrier config class"
      ],
      "metadata": {
        "id": "GyGd-YsQFR1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "\n",
        "def get_model_and_tokenizer(config: ProductionLoraConfig):\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit = config.load_in_4bit,\n",
        "        bnb_4bit_quant_type = config.bnb_4bit_quant_type,\n",
        "        bnb_4bit_use_double_quant = config.bnb_4bit_use_double_quant,\n",
        "        bnb_4bit_compute_dtype = config.bnb_4bit_compute_dtype\n",
        "    )\n",
        "\n",
        "    device_map = \"auto\" if torch.cuda.device_count() > 1 else None\n",
        "\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = config.model_name,\n",
        "        quantization_config = bnb_config,\n",
        "        max_seq_length = config.max_seq_length,\n",
        "        dtype = config.dtype,\n",
        "        device_map = device_map,\n",
        "        trust_remote_code = True\n",
        "    )\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "AqS6NATaGa4t"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2.3. Inject LoRA in the model using <font color = #5577FF>unsloth.FastLanguageModel.<b>get_peft_model(...)</b></font>"
      ],
      "metadata": {
        "id": "_qGKMtXTGbzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inject_lora_in_model(model: FastLanguageModel, config: ProductionLoraConfig):\n",
        "    lora_injected_model: FastLanguageModel = FastLanguageModel.get_peft_model(\n",
        "        model = model,\n",
        "        r = config.lora_r,\n",
        "        target_modules = config.target_modules,\n",
        "        lora_alpha = config.lora_alpha,\n",
        "        lora_dropout = config.lora_dropout,\n",
        "        bias = \"none\",\n",
        "        use_gradient_checkpointing = \"unsloth\",\n",
        "        random_state = 42,\n",
        "        max_seq_length = config.max_seq_length,\n",
        "        use_rslora = False # Don't use Rank Swappable LoRA (extreme memory savings on huge models)\n",
        "    )\n",
        "\n",
        "    if config.gradient_checkpointing is not None:\n",
        "        lora_injected_model.gradient_checkpointing_enable()\n",
        "\n",
        "    lora_injected_model.print_trainable_parameters()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        device_count: int = torch.cuda.device_count()\n",
        "\n",
        "        for i in range(device_count):\n",
        "            allocated = torch.cuda.memory_allocated(i) / gigs\n",
        "            cached = torch.cuda.memory_reserved(i) / gigs\n",
        "            print(f\"Memory allocated for GPU {Fore.MAGENTA}{i}{Style.RESET_ALL}:\\n\\tAllocated: {Style.BRIGHT}{Fore.GREEN}{allocated:.2f} GB{Style.RESET_ALL}\\n\\tCached: {Style.BRIGHT}{Fore.GREEN}{cached:.2f} GB{Style.RESET_ALL}\")\n",
        "\n",
        "        print(\"Model and Tokenizer loaded with LoRA config\")\n",
        "\n",
        "    return lora_injected_model"
      ],
      "metadata": {
        "id": "318GCWnGHSUI"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2.4. Train with unsloth"
      ],
      "metadata": {
        "id": "PtSlNSeEJlPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainerCallback\n",
        "\n",
        "\n",
        "class ProductionTrainerCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.start_time = None\n",
        "        self.best_loss = float('inf')\n",
        "        self.time_format = \"%Y-%m-%d %H:%M:%S\"\n",
        "\n",
        "\n",
        "    def on_train_begin(self, args, state, control, **kwargs):\n",
        "        self.start_time = datetime.now()\n",
        "\n",
        "        print(f\"Training started at {Fore.GREEN}{self.start_time.strftime(self.time_format)}{Style.RESET_ALL}\")\n",
        "\n",
        "\n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        end_time = datetime.now()\n",
        "        print(f\"Training completed at {Fore.GREEN}{end_time.strftime(self.time_format)}{Style.RESET_ALL}\")\n",
        "\n",
        "        duration = end_time - self.start_time\n",
        "        print(f\"Training duration: {Fore.GREEN}{duration}{Style.RESET_ALL}\")\n",
        "\n",
        "        print(f\"Best loss: {Fore.GREEN}{self.best_loss}{Style.RESET_ALL}\")\n",
        "\n",
        "\n",
        "    def on_log(self, args, state, control, logs = None, **kwargs):\n",
        "        if logs is not None:\n",
        "            current_learning_rate = logs.get(\"learning_rate\", -1)\n",
        "            train_loss = logs.get(\"loss\", -1) # TODO: or is it just loss?\n",
        "\n",
        "            if train_loss < self.best_loss:\n",
        "                self.best_loss = train_loss\n",
        "\n",
        "            print(f\"Step {Fore.RED}{state.global_step:>4d}{Style.RESET_ALL} | Loss: {Fore.GREEN}{train_loss:.4f}{Style.RESET_ALL} | LR: {Fore.GREEN}{current_learning_rate:.2e}{Style.RESET_ALL} | Best Loss: {Fore.GREEN}{self.best_loss:.4f}{Style.RESET_ALL}\")\n",
        "\n",
        "            if torch.cuda.is_available() and state.global_step % 50 == 0:\n",
        "                for i in range(torch.cuda.device_count()):\n",
        "                    allocated = torch.cuda.memory_allocated(i) / gigs\n",
        "\n",
        "                    print(f\"GPU {Fore.MAGENTA}{i}{Style.RESET_ALL}\\tMemory allocated: {Fore.GREEN}{allocated:.2f}{Style.RESET_ALL}GB\")\n",
        "\n",
        "\n",
        "    def on_evaluate(self, args, state, control, logs = None, **kwargs):\n",
        "        if logs is not None:\n",
        "            eval_loss = logs.get(\"eval_loss\", -1)\n",
        "\n",
        "            print(f\"Step {Fore.RED}{state.global_step}{Style.RESET_ALL} | Evaluation Loss: {Fore.GREEN}{eval_loss:.4f}{Style.RESET_ALL} | Best Loss: {Fore.GREEN}{self.best_loss:.4f}{Style.RESET_ALL}\")"
      ],
      "metadata": {
        "id": "u-fZhTzHJrOJ"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "\n",
        "\n",
        "def create_production_trainer(model, tokenizer, train_dataset, eval_dataset, config:ProductionLoraConfig):\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    output_dir: str = f\"./lora_production_{timestamp}\"\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir = output_dir,\n",
        "        overwrite_output_dir = True,\n",
        "        max_steps = config.max_steps,\n",
        "        per_device_train_batch_size = config.per_device_train_batch_size,\n",
        "        per_device_eval_batch_size = config.per_device_train_batch_size,\n",
        "        gradient_accumulation_steps = config.gradient_accumulation_steps,\n",
        "        learning_rate = config.learning_rate,\n",
        "        weight_decay = config.weight_decay,\n",
        "        warmup_ratio = config.warmup_ratio,\n",
        "        warmup_steps = config.warmup_steps,\n",
        "        optim = config.optimizer,\n",
        "        eval_strategy = \"no\",\n",
        "        save_strategy = \"steps\",\n",
        "        save_steps = config.save_steps,\n",
        "        save_total_limit = config.save_total_limit,\n",
        "        load_best_model_at_end = False, # Temporary fix for colab. avoid conflict with Supervised Fine Tuning trainer with multi processes.\n",
        "        logging_strategy = \"steps\",\n",
        "        logging_steps = config.logging_steps,\n",
        "        report_to = \"none\",\n",
        "        gradient_checkpointing = config.gradient_checkpointing,\n",
        "        dataloader_num_workers = 0,\n",
        "        remove_unused_columns = False,\n",
        "        group_by_length = False,\n",
        "\n",
        "        #Multi GPU training\n",
        "        ddp_find_unused_parameters = config.ddp_find_unused_parameters,\n",
        "\n",
        "        fp16 = config.dtype == torch.float16,\n",
        "        bf16 = config.dtype == torch.bfloat16,\n",
        "\n",
        "        dataloader_pin_memory = True,\n",
        "        skip_memory_metrics = False\n",
        "    )\n",
        "\n",
        "    callbacks = [ProductionTrainerCallback()]\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model = model,\n",
        "        train_dataset = train_dataset,\n",
        "        args = training_args,\n",
        "        tokenizer = tokenizer,\n",
        "        callbacks = callbacks,\n",
        "        max_seq_length = config.max_seq_length\n",
        "    )\n",
        "\n",
        "    return trainer, output_dir"
      ],
      "metadata": {
        "id": "cfZyiyQgq9hu"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2.5. Train production model method"
      ],
      "metadata": {
        "id": "dWat0tLcvrzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.5.1. Step 1: Dataset preparation code"
      ],
      "metadata": {
        "id": "WCxyLcBXw3ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(dataset_name: str, model_name: str, max_samples: int, max_seq_length: int):\n",
        "    tokenized_dataset, tokenizer = prepare_dataset_from_hugging_face(\n",
        "    dataset_name = dataset_name,\n",
        "    model_name = model_name,\n",
        "    max_samples = max_samples,\n",
        "    max_length = max_seq_length\n",
        "    )\n",
        "\n",
        "    train_dataset = tokenized_dataset\n",
        "    eval_dataset = None\n",
        "\n",
        "    return train_dataset, eval_dataset, tokenizer"
      ],
      "metadata": {
        "id": "2RmTr7JbwpjR"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.5.2. Step 5: Save objects\n",
        "Notes:\n",
        "<ul>\n",
        "    <li>Saves adapter and tokenizer.</li>\n",
        "    <li>Steps 2, 3, and 4 are in the training production model method</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "vIfFPxnE1WJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "\n",
        "def merge_models(model, root_dir):\n",
        "    adapter_dir = os.path.join(root_dir, \"adapter\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
        "    model = PeftModel.from_pretrained(model, adapter_dir)\n",
        "    model = model.merge_and_unload() # Merge the adapters, and release the memory\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def save_object(obj, root_dir, dir_name, is_sfttrainer: bool = False):\n",
        "    dir = os.path.join(root_dir, dir_name)\n",
        "    os.makedirs(dir, exist_ok = True)\n",
        "\n",
        "    if is_sfttrainer:\n",
        "        obj.save_model(dir)\n",
        "    else:\n",
        "        obj.save_pretrained(dir)\n",
        "\n",
        "\n",
        "def save_objects(model, tokenizer, trainer, root_dir):\n",
        "    save_object(model, root_dir, \"adapter\")\n",
        "    save_object(tokenizer, root_dir, \"tokenizer\")\n",
        "    save_object(trainer, root_dir, \"final_model\", True)\n",
        "\n",
        "\n",
        "def merge_and_save_objects(model, tokenizer, trainer, root_dir, merge_models: bool = False):\n",
        "    if merge_models:\n",
        "        model = merge_models(model, root_dir)\n",
        "\n",
        "    save_objects(model, tokenizer, trainer, root_dir)"
      ],
      "metadata": {
        "id": "anSucJ4A1Uvk"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.5.3. train production model method"
      ],
      "metadata": {
        "id": "xbtpXDmBwu_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_production_model(dataset_name: str = \"timdettmers/openassistant-guanco\", config: ProductionLoraConfig = None, max_samples: int = None, eval_split_ratio = 0.0):\n",
        "    if config is None:\n",
        "        config = ProductionLoraConfig()\n",
        "\n",
        "    print(\"LoRA pipeline started\")\n",
        "    config.print_config()\n",
        "\n",
        "    # Step 1: call dataset preparation code\n",
        "    train_dataset, eval_dataset, tokenizer = prepare_dataset(dataset_name, config.model_name, max_samples, config.max_seq_length)\n",
        "\n",
        "    # Step 2: load model\n",
        "    model, tokenizer = get_model_and_tokenizer(config)\n",
        "    model = inject_lora_in_model(model, config)\n",
        "\n",
        "    # Step 3: Training setup\n",
        "    trainer, output_dir = create_production_trainer(model, tokenizer, train_dataset, eval_dataset, config)\n",
        "\n",
        "    # Step 4: Train\n",
        "    trainer.train()\n",
        "\n",
        "    # Step 5: Save model\n",
        "    merge_and_save_objects(model, tokenizer, trainer, output_dir)\n",
        "\n",
        "    return trainer, output_dir"
      ],
      "metadata": {
        "id": "50HCOErmt_su"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.5.4. Run training code"
      ],
      "metadata": {
        "id": "Zmli5z417EcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "production_config = ProductionLoraConfig(\n",
        "    model_name = \"openai-community/gpt2\",\n",
        "    lora_r = 16,\n",
        "    lora_alpha = 16,\n",
        "    max_steps = max_steps,\n",
        "    per_device_train_batch_size = 2,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    learning_rate = 2e-4,\n",
        "    optimizer = \"adamw_8bit\",\n",
        "    warmup_steps = 100,\n",
        "    save_steps = 250,\n",
        "    logging_steps = 25,\n",
        "    load_in_4bit = True,\n",
        "    bnb_4bit_quant_type = \"nf4\",\n",
        "    dtype = torch.float16,\n",
        "    eval_strategy = \"no\",\n",
        "    eval_steps = None,\n",
        "    load_best_model_at_end = False\n",
        ")\n",
        "\n",
        "print(\"Production training pipeline started\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IcBvx-27A7w",
        "outputId": "4740eebb-47e9-42a1-f1b9-3148f6179d03"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Production training pipeline started\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer, output_dir = train_production_model(\n",
        "    dataset_name = \"timdettmers/openassistant-guanaco\",\n",
        "    config = production_config,\n",
        "    max_samples = 1000,\n",
        "    eval_split_ratio = 0.0\n",
        ")\n",
        "\n",
        "print(\"Production training pipeline ended\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TJpye-ro9jPX",
        "outputId": "b8a23631-fb9d-4144-a68a-a10d2da50673"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA pipeline started\n",
            "LoRA Configuration\n",
            "model_name\t\t\t\t\t: \u001b[1m\u001b[36mopenai-community/gpt2\u001b[0m\n",
            "max_seq_length\t\t\t\t: \u001b[1m\u001b[36m512\u001b[0m\n",
            "dtype\t\t\t\t\t\t: \u001b[1m\u001b[36mtorch.float16\u001b[0m\n",
            "lora_r\t\t\t\t\t\t: \u001b[1m\u001b[36m16\u001b[0m\n",
            "lora_alpha\t\t\t\t\t: \u001b[1m\u001b[36m16\u001b[0m\n",
            "lora_dropout\t\t\t\t: \u001b[1m\u001b[36m0.05\u001b[0m\n",
            "target_modules\t\t\t\t: \u001b[1m\u001b[36m['c_attn', 'c_proj']\u001b[0m\n",
            "load_in_4bit\t\t\t\t: \u001b[1m\u001b[36mTrue\u001b[0m\n",
            "bnb_4bit_compute_dtype\t\t: \u001b[1m\u001b[36mtorch.float16\u001b[0m\n",
            "bnb_4bit_quant_type\t\t\t: \u001b[1m\u001b[36mnf4\u001b[0m\n",
            "bnb_4bit_use_double_quant\t: \u001b[1m\u001b[36mTrue\u001b[0m\n",
            "bnb_8bit_quant_type\t\t\t: \u001b[1m\u001b[36mnf4\u001b[0m\n",
            "bnb_8bit_use_double_quant\t: \u001b[1m\u001b[36mTrue\u001b[0m\n",
            "bnb_8bit_compute_dtype\t\t: \u001b[1m\u001b[36mtorch.float16\u001b[0m\n",
            "max_steps\t\t\t\t\t: \u001b[1m\u001b[36m1000\u001b[0m\n",
            "per_device_train_batch_size\t: \u001b[1m\u001b[36m2\u001b[0m\n",
            "gradient_accumulation_steps\t: \u001b[1m\u001b[36m4\u001b[0m\n",
            "learning_rate\t\t\t\t: \u001b[1m\u001b[36m0.0002\u001b[0m\n",
            "weight_decay\t\t\t\t: \u001b[1m\u001b[36m0.001\u001b[0m\n",
            "warmup_steps\t\t\t\t: \u001b[1m\u001b[36m100\u001b[0m\n",
            "warmup_ratio\t\t\t\t: \u001b[1m\u001b[36m0.1\u001b[0m\n",
            "optimizer\t\t\t\t\t: \u001b[1m\u001b[36madamw_8bit\u001b[0m\n",
            "save_steps\t\t\t\t\t: \u001b[1m\u001b[36m250\u001b[0m\n",
            "save_total_limit\t\t\t: \u001b[1m\u001b[36m3\u001b[0m\n",
            "eval_strategy\t\t\t\t: \u001b[1m\u001b[36mno\u001b[0m\n",
            "eval_steps\t\t\t\t\t: \u001b[1m\u001b[36mNone\u001b[0m\n",
            "load_best_model_at_end\t\t: \u001b[1m\u001b[36mFalse\u001b[0m\n",
            "metric_for_best_model\t\t: \u001b[1m\u001b[36mloss\u001b[0m\n",
            "greater_is_better\t\t\t: \u001b[1m\u001b[36mFalse\u001b[0m\n",
            "gradient_checkpointing\t\t: \u001b[1m\u001b[36mTrue\u001b[0m\n",
            "dataloader_num_workers\t\t: \u001b[1m\u001b[36m4\u001b[0m\n",
            "remove_unused_columns\t\t: \u001b[1m\u001b[36mFalse\u001b[0m\n",
            "group_by_length\t\t\t\t: \u001b[1m\u001b[36mTrue\u001b[0m\n",
            "ddp_find_unused_parameters\t: \u001b[1m\u001b[36mFalse\u001b[0m\n",
            "logging_steps\t\t\t\t: \u001b[1m\u001b[36m25\u001b[0m\n",
            "report_to\t\t\t\t\t: \u001b[1m\u001b[36mtensorboard\u001b[0m\n",
            "\n",
            "Effective Batch Size          \t: \u001b[1m\u001b[36m8\u001b[0m across \u001b[1m\u001b[31m1\u001b[0m GPU(s)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset \u001b[94mtimdettmers/openassistant-guanaco\u001b[0m loaded successfully.\n",
            "Tokenizer \u001b[94mopenai-community/gpt2\u001b[0m loaded successfully.\n",
            "\u001b[36mTokenization started\u001b[0m\n",
            "\u001b[36mTokenization completed\u001b[0m\n",
            "Unsloth: WARNING `trust_remote_code` is True.\n",
            "Are you certain you want to do remote code execution?\n",
            "==((====))==  Unsloth 2025.6.2: Fast Gpt2 patching. Transformers: 4.52.4.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Gpt2 does not support SDPA - switching to eager!\n",
            "openai-community/gpt2 does not have a padding token! Will use pad_token = <|endoftext|>.\n",
            "Unsloth: Making `model.base_model.model.transformer` require gradients\n",
            "trainable params: 1,622,016 || all params: 126,061,824 || trainable%: 1.2867\n",
            "Memory allocated for GPU \u001b[35m0\u001b[0m:\n",
            "\tAllocated: \u001b[1m\u001b[32m0.51 GB\u001b[0m\n",
            "\tCached: \u001b[1m\u001b[32m0.62 GB\u001b[0m\n",
            "Model and Tokenizer loaded with LoRA config\n",
            "Training started at \u001b[32m2025-06-15 03:05:08\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 37:18, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>26.071000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>15.625800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>11.153700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>9.610400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>9.597200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>9.549800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>9.534500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>9.369800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>9.534600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>9.824000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>9.564600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>9.566500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>8.789500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>9.081000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>9.617900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>9.382100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>425</td>\n",
              "      <td>8.904700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>9.169100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>475</td>\n",
              "      <td>9.156600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>9.416700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>525</td>\n",
              "      <td>9.302600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>9.032900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>8.973800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>9.184700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>625</td>\n",
              "      <td>9.206000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>8.615600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>675</td>\n",
              "      <td>9.703200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>8.989200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>725</td>\n",
              "      <td>9.351500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>8.807400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>775</td>\n",
              "      <td>8.559700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>9.118000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>825</td>\n",
              "      <td>9.582900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>9.021600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>875</td>\n",
              "      <td>9.029800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>8.771900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>925</td>\n",
              "      <td>9.327100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>8.982100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>975</td>\n",
              "      <td>8.970200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>9.158500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step \u001b[31m  25\u001b[0m | Loss: \u001b[32m26.0710\u001b[0m | LR: Loss: \u001b[32m4.80e-05\u001b[0m | Best Loss: \u001b[32m26.0710\u001b[0m\n",
            "Step \u001b[31m  50\u001b[0m | Loss: \u001b[32m15.6258\u001b[0m | LR: Loss: \u001b[32m9.80e-05\u001b[0m | Best Loss: \u001b[32m15.6258\u001b[0m\n",
            "GPU \u001b[35m0\u001b[0m\tMemory allocated: \u001b[32m0.51\u001b[0mGB\n",
            "Step \u001b[31m  75\u001b[0m | Loss: \u001b[32m11.1537\u001b[0m | LR: Loss: \u001b[32m1.48e-04\u001b[0m | Best Loss: \u001b[32m11.1537\u001b[0m\n",
            "Step \u001b[31m 100\u001b[0m | Loss: \u001b[32m9.6104\u001b[0m | LR: Loss: \u001b[32m1.98e-04\u001b[0m | Best Loss: \u001b[32m9.6104\u001b[0m\n",
            "GPU \u001b[35m0\u001b[0m\tMemory allocated: \u001b[32m0.51\u001b[0mGB\n",
            "Step \u001b[31m 125\u001b[0m | Loss: \u001b[32m9.5972\u001b[0m | LR: Loss: \u001b[32m1.95e-04\u001b[0m | Best Loss: \u001b[32m9.5972\u001b[0m\n",
            "Step \u001b[31m 150\u001b[0m | Loss: \u001b[32m9.5498\u001b[0m | LR: Loss: \u001b[32m1.89e-04\u001b[0m | Best Loss: \u001b[32m9.5498\u001b[0m\n",
            "GPU \u001b[35m0\u001b[0m\tMemory allocated: \u001b[32m0.51\u001b[0mGB\n",
            "Step \u001b[31m 175\u001b[0m | Loss: \u001b[32m9.5345\u001b[0m | LR: Loss: \u001b[32m1.84e-04\u001b[0m | Best Loss: \u001b[32m9.5345\u001b[0m\n",
            "Step \u001b[31m 200\u001b[0m | Loss: \u001b[32m9.3698\u001b[0m | LR: Loss: \u001b[32m1.78e-04\u001b[0m | Best Loss: \u001b[32m9.3698\u001b[0m\n",
            "GPU \u001b[35m0\u001b[0m\tMemory allocated: \u001b[32m0.51\u001b[0mGB\n",
            "Step \u001b[31m 225\u001b[0m | Loss: \u001b[32m9.5346\u001b[0m | LR: Loss: \u001b[32m1.72e-04\u001b[0m | Best Loss: \u001b[32m9.3698\u001b[0m\n",
            "Step \u001b[31m 250\u001b[0m | Loss: \u001b[32m9.8240\u001b[0m | LR: Loss: \u001b[32m1.67e-04\u001b[0m | Best Loss: \u001b[32m9.3698\u001b[0m\n",
            "GPU \u001b[35m0\u001b[0m\tMemory allocated: \u001b[32m0.51\u001b[0mGB\n",
            "Step \u001b[31m 275\u001b[0m | Loss: \u001b[32m9.5646\u001b[0m | LR: Loss: \u001b[32m1.61e-04\u001b[0m | Best Loss: \u001b[32m9.3698\u001b[0m\n",
            "Step \u001b[31m 300\u001b[0m | Loss: \u001b[32m9.5665\u001b[0m | LR: Loss: \u001b[32m1.56e-04\u001b[0m | Best Loss: \u001b[32m9.3698\u001b[0m\n",
            "GPU \u001b[35m0\u001b[0m\tMemory allocated: \u001b[32m0.51\u001b[0mGB\n",
            "Step \u001b[31m 325\u001b[0m | Loss: \u001b[32m8.7895\u001b[0m | LR: Loss: \u001b[32m1.50e-04\u001b[0m | Best Loss: \u001b[32m8.7895\u001b[0m\n",
            "Step \u001b[31m 350\u001b[0m | Loss: \u001b[32m9.0810\u001b[0m | LR: Loss: \u001b[32m1.45e-04\u001b[0m | Best Loss: \u001b[32m8.7895\u001b[0m\n",
            "GPU \u001b[35m0\u001b[0m\tMemory allocated: \u001b[32m0.51\u001b[0mGB\n",
            "Step \u001b[31m 375\u001b[0m | Loss: \u001b[32m9.6179\u001b[0m | LR: Loss: \u001b[32m1.39e-04\u001b[0m | Best Loss: \u001b[32m8.7895\u001b[0m\n",
            "Step \u001b[31m 400\u001b[0m | Loss: \u001b[32m9.3821\u001b[0m | LR: Loss: \u001b[32m1.34e-04\u001b[0m | Best Loss: \u001b[32m8.7895\u001b[0m\n",
            "GPU \u001b[35m0\u001b[0m\tMemory allocated: \u001b[32m0.51\u001b[0mGB\n",
            "Step \u001b[31m 425\u001b[0m | Loss: \u001b[32m8.9047\u001b[0m | LR: Loss: \u001b[32m1.28e-04\u001b[0m | Best Loss: \u001b[32m8.7895\u001b[0m\n",
            "Step \u001b[31m 450\u001b[0m | Loss: \u001b[32m9.1691\u001b[0m | LR: Loss: \u001b[32m1.22e-04\u001b[0m | Best Loss: \u001b[32m8.7895\u001b[0m\n",
            "GPU \u001b[35m0\u001b[0m\tMemory allocated: \u001b[32m0.51\u001b[0mGB\n",
            "Step \u001b[31m 475\u001b[0m | Loss: \u001b[32m9.1566\u001b[0m | LR: Loss: \u001b[32m1.17e-04\u001b[0m | Best Loss: \u001b[32m8.7895\u001b[0m\n",
            "Step \u001b[31m 500\u001b[0m | Loss: \u001b[32m9.4167\u001b[0m | LR: Loss: \u001b[32m1.11e-04\u001b[0m | Best Loss: \u001b[32m8.7895\u001b[0m\n",
            "GPU \u001b[35m0\u001b[0m\tMemory allocated: \u001b[32m0.51\u001b[0mGB\n",
            "Step \u001b[31m 525\u001b[0m | Loss: \u001b[32m9.3026\u001b[0m | LR: Loss: \u001b[32m1.06e-04\u001b[0m | Best Loss: \u001b[32m8.7895\u001b[0m\n",
            "Step \u001b[31m 550\u001b[0m | Loss: \u001b[32m9.0329\u001b[0m | LR: Loss: \u001b[32m1.00e-04\u001b[0m | Best Loss: \u001b[32m8.7895\u001b[0m\n",
            "GPU \u001b[35m0\u001b[0m\tMemory allocated: \u001b[32m0.51\u001b[0mGB\n",
            "Step \u001b[31m 575\u001b[0m | Loss: \u001b[32m8.9738\u001b[0m | LR: Loss: \u001b[32m9.47e-05\u001b[0m | Best Loss: \u001b[32m8.7895\u001b[0m\n",
            "Step \u001b[31m 600\u001b[0m | Loss: \u001b[32m9.1847\u001b[0m | LR: Loss: \u001b[32m8.91e-05\u001b[0m | Best Loss: \u001b[32m8.7895\u001b[0m\n",
            "GPU \u001b[35m0\u001b[0m\tMemory allocated: \u001b[32m0.51\u001b[0mGB\n",
            "Step \u001b[31m 625\u001b[0m | Loss: \u001b[32m9.2060\u001b[0m | LR: Loss: \u001b[32m8.36e-05\u001b[0m | Best Loss: \u001b[32m8.7895\u001b[0m\n",
            "Step \u001b[31m 650\u001b[0m | Loss: \u001b[32m8.6156\u001b[0m | LR: Loss: \u001b[32m7.80e-05\u001b[0m | Best Loss: \u001b[32m8.6156\u001b[0m\n",
            "GPU \u001b[35m0\u001b[0m\tMemory allocated: \u001b[32m0.51\u001b[0mGB\n",
            "Step \u001b[31m 675\u001b[0m | Loss: \u001b[32m9.7032\u001b[0m | LR: Loss: \u001b[32m7.24e-05\u001b[0m | Best Loss: \u001b[32m8.6156\u001b[0m\n",
            "Step \u001b[31m 700\u001b[0m | Loss: \u001b[32m8.9892\u001b[0m | LR: Loss: \u001b[32m6.69e-05\u001b[0m | Best Loss: \u001b[32m8.6156\u001b[0m\n",
            "GPU \u001b[35m0\u001b[0m\tMemory allocated: \u001b[32m0.51\u001b[0mGB\n",
            "Step \u001b[31m 725\u001b[0m | Loss: \u001b[32m9.3515\u001b[0m | LR: Loss: \u001b[32m6.13e-05\u001b[0m | Best Loss: \u001b[32m8.6156\u001b[0m\n",
            "Step \u001b[31m 750\u001b[0m | Loss: \u001b[32m8.8074\u001b[0m | LR: Loss: \u001b[32m5.58e-05\u001b[0m | Best Loss: \u001b[32m8.6156\u001b[0m\n",
            "GPU \u001b[35m0\u001b[0m\tMemory allocated: \u001b[32m0.51\u001b[0mGB\n",
            "Step \u001b[31m 775\u001b[0m | Loss: \u001b[32m8.5597\u001b[0m | LR: Loss: \u001b[32m5.02e-05\u001b[0m | Best Loss: \u001b[32m8.5597\u001b[0m\n",
            "Step \u001b[31m 800\u001b[0m | Loss: \u001b[32m9.1180\u001b[0m | LR: Loss: \u001b[32m4.47e-05\u001b[0m | Best Loss: \u001b[32m8.5597\u001b[0m\n",
            "GPU \u001b[35m0\u001b[0m\tMemory allocated: \u001b[32m0.51\u001b[0mGB\n",
            "Step \u001b[31m 825\u001b[0m | Loss: \u001b[32m9.5829\u001b[0m | LR: Loss: \u001b[32m3.91e-05\u001b[0m | Best Loss: \u001b[32m8.5597\u001b[0m\n",
            "Step \u001b[31m 850\u001b[0m | Loss: \u001b[32m9.0216\u001b[0m | LR: Loss: \u001b[32m3.36e-05\u001b[0m | Best Loss: \u001b[32m8.5597\u001b[0m\n",
            "GPU \u001b[35m0\u001b[0m\tMemory allocated: \u001b[32m0.51\u001b[0mGB\n",
            "Step \u001b[31m 875\u001b[0m | Loss: \u001b[32m9.0298\u001b[0m | LR: Loss: \u001b[32m2.80e-05\u001b[0m | Best Loss: \u001b[32m8.5597\u001b[0m\n",
            "Step \u001b[31m 900\u001b[0m | Loss: \u001b[32m8.7719\u001b[0m | LR: Loss: \u001b[32m2.24e-05\u001b[0m | Best Loss: \u001b[32m8.5597\u001b[0m\n",
            "GPU \u001b[35m0\u001b[0m\tMemory allocated: \u001b[32m0.51\u001b[0mGB\n",
            "Step \u001b[31m 925\u001b[0m | Loss: \u001b[32m9.3271\u001b[0m | LR: Loss: \u001b[32m1.69e-05\u001b[0m | Best Loss: \u001b[32m8.5597\u001b[0m\n",
            "Step \u001b[31m 950\u001b[0m | Loss: \u001b[32m8.9821\u001b[0m | LR: Loss: \u001b[32m1.13e-05\u001b[0m | Best Loss: \u001b[32m8.5597\u001b[0m\n",
            "GPU \u001b[35m0\u001b[0m\tMemory allocated: \u001b[32m0.51\u001b[0mGB\n",
            "Step \u001b[31m 975\u001b[0m | Loss: \u001b[32m8.9702\u001b[0m | LR: Loss: \u001b[32m5.78e-06\u001b[0m | Best Loss: \u001b[32m8.5597\u001b[0m\n",
            "Step \u001b[31m1000\u001b[0m | Loss: \u001b[32m9.1585\u001b[0m | LR: Loss: \u001b[32m2.22e-07\u001b[0m | Best Loss: \u001b[32m8.5597\u001b[0m\n",
            "GPU \u001b[35m0\u001b[0m\tMemory allocated: \u001b[32m0.51\u001b[0mGB\n",
            "Step \u001b[31m1000\u001b[0m | Loss: \u001b[32m-1.0000\u001b[0m | LR: Loss: \u001b[32m-1.00e+00\u001b[0m | Best Loss: \u001b[32m-1.0000\u001b[0m\n",
            "GPU \u001b[35m0\u001b[0m\tMemory allocated: \u001b[32m0.51\u001b[0mGB\n",
            "Training completed at \u001b[32m2025-06-15 03:42:29\u001b[0m\n",
            "Training duration: \u001b[32m0:37:20.709826\u001b[0m\n",
            "Best loss: \u001b[32m-1\u001b[0m\n",
            "Production training pipeline ended\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3. Infer"
      ],
      "metadata": {
        "id": "2gRhzb1RN-zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "jnPwoq_lSIA5"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"The future of AI is\",\n",
        "    \"The story of Terminator 2 movie is\",\n",
        "    \"The most important lesson from the story of David and Goliath is\",\n",
        "    \"Climate change represents a challenge that\",\n",
        "    \"Innovation in healthcare could lead to\"\n",
        "]"
      ],
      "metadata": {
        "id": "LixZaPG_SFiJ"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "import torch\n",
        "\n",
        "class Completer:\n",
        "    def __init__(self, model_path):\n",
        "        self.model_path = model_path\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(self.model_path)\n",
        "        print(\"Final model loaded\")\n",
        "\n",
        "        self.generator = pipeline(\n",
        "            task = \"text-generation\",\n",
        "            model = self.model,\n",
        "            tokenizer = self.tokenizer,\n",
        "            max_new_tokens = 200,\n",
        "            device = 0 if torch.cuda.is_available() else -1,\n",
        "            pad_token_id = self.tokenizer.eos_token_id,\n",
        "            bos_token_id = self.tokenizer.eos_token_id # Set bos_token_id here as well\n",
        "        )\n",
        "\n",
        "        self.generation_config: dict = {\n",
        "            \"max_length\": 400,\n",
        "            \"num_return_sequences\": 1,\n",
        "            \"temperature\": 0.7,\n",
        "            \"do_sample\": True,\n",
        "            \"pad_token_id\": self.tokenizer.eos_token_id,\n",
        "            \"eos_token_id\": self.tokenizer.eos_token_id,\n",
        "            \"bos_token_id\": self.tokenizer.eos_token_id\n",
        "        }\n",
        "\n",
        "\n",
        "    def complete_prompt(self, prompt: str) -> str:\n",
        "        # Convert the dictionary to a GenerationConfig object\n",
        "        gen_config = GenerationConfig.from_dict(self.generation_config)\n",
        "        outputs = self.generator(prompt, generation_config = gen_config)\n",
        "        generated_text = outputs[0][\"generated_text\"]\n",
        "\n",
        "        return generated_text"
      ],
      "metadata": {
        "id": "i1REAIqnVF2p"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"./lora_production_2025-06-15_03-05-06/final_model\"\n",
        "\n",
        "text_completer: Completer = Completer(model_path)\n",
        "print()\n",
        "\n",
        "for i, prompt in enumerate(prompts):\n",
        "    print(f\"Prompt {Fore.LIGHTRED_EX}{i}{Style.RESET_ALL}:\\n\\t{Fore.BLUE}{prompt}...{Style.RESET_ALL}\")\n",
        "    generated_text: str = text_completer.complete_prompt(prompt)\n",
        "    print(f\"\\nCompleted prompt:\\n\\t{Style.BRIGHT}{Fore.GREEN}{generated_text}{Style.RESET_ALL}\\n\\n\")\n",
        "\n",
        "print(\"Code completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxeA_UBnN93c",
        "outputId": "93f0d5aa-77b4-4a61-dc75-e724d39565ad"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final model loaded\n",
            "\n",
            "Prompt \u001b[91m0\u001b[0m:\n",
            "\t\u001b[34mThe future of AI is...\u001b[0m\n",
            "\n",
            "Completed prompt:\n",
            "\t\u001b[1m\u001b[32mThe future of AI is always up for debate.\u001b[0m\n",
            "\n",
            "\n",
            "Prompt \u001b[91m1\u001b[0m:\n",
            "\t\u001b[34mThe story of Terminator 2 movie is...\u001b[0m\n",
            "\n",
            "Completed prompt:\n",
            "\t\u001b[1m\u001b[32mThe story of Terminator 2 movie is a story of a robot who is trying to take over the world and kill humans. The robot tries to kill humans, but the humans are able to stop him and make him believe he is the only one left.\n",
            "\n",
            "The real Terminator 3 is a story of a robot who has gone rogue and is trying to save humanity from a mysterious force.\n",
            "\n",
            "The real Terminator 4 is a story of a robot who is trying to take over the world and kill humans. The robot tries to save humanity from a mysterious force.\u001b[0m\n",
            "\n",
            "\n",
            "Prompt \u001b[91m2\u001b[0m:\n",
            "\t\u001b[34mThe most important lesson from the story of David and Goliath is...\u001b[0m\n",
            "\n",
            "Completed prompt:\n",
            "\t\u001b[1m\u001b[32mThe most important lesson from the story of David and Goliath is to not dwell on the small things. They had nothing to lose. They had everything to gain. Now, they had nothing to lose. They had nothing to gain.\n",
            "\n",
            "David and Goliath had nothing to lose. They had nothing to gain. Now, they had nothing to gain.\n",
            "\n",
            "Goliath, the god of destruction, was as good as David and Goliath had to lose. The only thing they had to gain was their lives. David and Goliath had nothing to lose and they had nothing to gain.\n",
            "\n",
            "Now, David and Goliath had nothing to gain, because they didn't have anything to lose. They had nothing to gain because they didn't have anything to lose. They had nothing to gain because they didn't have anything to lose.\u001b[0m\n",
            "\n",
            "\n",
            "Prompt \u001b[91m3\u001b[0m:\n",
            "\t\u001b[34mClimate change represents a challenge that...\u001b[0m\n",
            "\n",
            "Completed prompt:\n",
            "\t\u001b[1m\u001b[32mClimate change represents a challenge that can be overcome only by working with scientists who understand the underlying mechanisms behind climate change.\n",
            "\n",
            "###\n",
            "\n",
            "The purpose of this work is to present a computational model of climate change using a mathematical approach based on the mathematical model of linear evolution. The model is described as a mathematical approach that can be used to perform a series of mathematical transformations in an iterative fashion. The models are computed using a generalized method that involves the following assumptions:\n",
            "\n",
            "- The probability of the model being true\n",
            "- The number of events in a given period, e.g., the event of the week or month, i.e., the number of significant events in a given day, e.g., the number of significant events in a day, i.e., the number of significant events in a day, i.e., the number of significant events in a day, i.e., the number of significant events in a day.\n",
            "- The number of times a given variable is changed, e.g., the number of different variables in a given period, e.g., the number of different periods in a given year, e.g., the number of different periods in a given month, e.g., the number of different periods in a given year, or the number of different periods in a given year.\n",
            "- The number of events in a given month, e.g., the number of different events in a given year, e.g., the number of different periods in a given month, e.g., the number of different periods in a given year, or the number of different periods in a given year.\n",
            "- The number of distinct events in a given year, e.g., the number of distinct events in a given month, e.g., the number of distinct events in a given year, or the number of distinct events in a given year, or the number of distinct events in a given year, or the\u001b[0m\n",
            "\n",
            "\n",
            "Prompt \u001b[91m4\u001b[0m:\n",
            "\t\u001b[34mInnovation in healthcare could lead to...\u001b[0m\n",
            "\n",
            "Completed prompt:\n",
            "\t\u001b[1m\u001b[32mInnovation in healthcare could lead to better outcomes for those with disabilities and those who suffer from chronic diseases.\n",
            "\n",
            "###\u001b[0m\n",
            "\n",
            "\n",
            "Code completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END"
      ],
      "metadata": {
        "id": "yJAAY7Cc9m6z"
      }
    }
  ]
}