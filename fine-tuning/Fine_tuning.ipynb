{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "uTfEComlSKtZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Introduction\n",
        "Fine tune an LLM using Huggingface platform"
      ],
      "metadata": {
        "id": "3ebtz2EJMJfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install\n",
        "_Run once_"
      ],
      "metadata": {
        "id": "nXcF3J4ZMrO_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5Lpx1tn7C0uT"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "\n",
        "!pip install unsloth\n",
        "!pip install \"torch >= 2.0.0\"\n",
        "!pip install \"bitsandbytes >= 0.40.0\"\n",
        "!pip install \"transformers >= 4.30.0\"\n",
        "!pip install \"accelerate >= 0.20.3\"\n",
        "!pip install \"datasets >= 2.12.0\"\n",
        "!pip install \"trl > 0.6.0\"\n",
        "!pip install \"unsloth >= 0.1.0\"\n",
        "!pip install \"peft >= 0.4.0\"\n",
        "!pip install \"evaluate\"\n",
        "!pip install \"tensorboard\"\n",
        "\n",
        "!pip install colorama"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Imports"
      ],
      "metadata": {
        "id": "8eRSk6eKNzUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unsloth\n",
        "import torch\n",
        "import bitsandbytes as bnb\n",
        "import transformers\n",
        "import accelerate\n",
        "import datasets as ds\n",
        "import trl\n",
        "\n",
        "import os\n",
        "\n",
        "from colorama import Fore, Back, Style"
      ],
      "metadata": {
        "id": "CDo50Oy9Nx5x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "803699d5-1421-4877-d802-f562a9e5a496"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'unsloth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-735c89e06468>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0munsloth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitsandbytes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccelerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unsloth'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Check versions"
      ],
      "metadata": {
        "id": "6DUXsgv90R1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Unsloth Version\\t\\t: {Style.BRIGHT}{Fore.CYAN}{unsloth.__version__}{Style.RESET_ALL}\")\n",
        "print(f\"Torch Version\\t\\t: {Style.BRIGHT}{Fore.CYAN}{torch.__version__}{Style.RESET_ALL}\")\n",
        "print(f\"BitsAndBytes Version\\t: {Style.BRIGHT}{Fore.CYAN}{bnb.__version__}{Style.RESET_ALL}\")\n",
        "print(f\"Transformers Version\\t: {Style.BRIGHT}{Fore.CYAN}{transformers.__version__}{Style.RESET_ALL}\")\n",
        "print(f\"Accelerate Version\\t: {Style.BRIGHT}{Fore.CYAN}{accelerate.__version__}{Style.RESET_ALL}\")\n",
        "print(f\"Datasets Version\\t: {Style.BRIGHT}{Fore.CYAN}{ds.__version__}{Style.RESET_ALL}\")\n",
        "print(f\"TRL Version\\t\\t: {Style.BRIGHT}{Fore.CYAN}{trl.__version__}{Style.RESET_ALL}\")\n",
        "print(f\"\\nCUDA Availability\\t: {Style.BRIGHT}{Fore.GREEN}{torch.cuda.is_available()}{Style.RESET_ALL}\")\n",
        "print(f\"CUDA Device Count\\t: {Style.BRIGHT}{Fore.GREEN}{torch.cuda.device_count()}{Style.RESET_ALL}\")"
      ],
      "metadata": {
        "id": "SwCQKhwXzyAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Variables"
      ],
      "metadata": {
        "id": "eJjP8Zpn3Wd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Environment Variables"
      ],
      "metadata": {
        "id": "_H9NeYfH3Z_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configured env var for Unsloth <---- particularly for colab notebook\n",
        "os.environ[\"UNSLOTH_COMPILE_DISABLE\"] = \"1\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "metadata": {
        "id": "UL3K_ytM1pGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. User Variables"
      ],
      "metadata": {
        "id": "nYjlngg2_bla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name: str = \"distilgpt2\"\n",
        "dataset_name: str = \"timdettmers/openassistant-guanaco\""
      ],
      "metadata": {
        "id": "JFmtxYk4_h7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Code"
      ],
      "metadata": {
        "id": "8YnojQ-Y_T1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Prepare dataset\n",
        "Steps:\n",
        "<ol>\n",
        "    <li>Fetch a dataset</li>\n",
        "    <li>Put an <font color=\"#22819F\"><b>eos_token</b></font> if the examples do not have one at the end</li>\n",
        "    <li>For tokenized dataset, change token ids to a reserved keyword (<i>-100 here</i>) for pad tokens</li>\n",
        "    <li>Tokenize</li>\n",
        "</ol>"
      ],
      "metadata": {
        "id": "uTfEComlSKtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "from datasets import DatasetDict\n",
        "from datasets import IterableDataset\n",
        "from datasets import IterableDatasetDict\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from typing import Any"
      ],
      "metadata": {
        "id": "XRWfRpAh30vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.1. Fetch a dataset\n",
        "(from Hugging face)"
      ],
      "metadata": {
        "id": "djAVmSVxWCld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(dataset_name: str, max_samples: int\n",
        "    ) -> Dataset | DatasetDict | IterableDataset | IterableDatasetDict:\n",
        "    \"\"\"\n",
        "    Get dataset from HuggingFace.\n",
        "\n",
        "    Args:\n",
        "        dataset_name (str): Name of the dataset.\n",
        "        max_samples (int): Maximum number of samples in the dataset to use.\n",
        "\n",
        "    Returns:\n",
        "        Dataset | DatasetDict | IterableDataset | IterableDatasetDict\n",
        "    \"\"\"\n",
        "\n",
        "    dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "    if max_samples is not None and max_samples < len(dataset):\n",
        "        dataset = dataset.select(range(max_samples))\n",
        "\n",
        "    print(f\"Dataset {Fore.LIGHTBLUE_EX}{dataset_name}{Style.RESET_ALL} loaded successfully.\")\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "dH9Oio1FWL6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenizer(model_name: str) -> Any:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    print(f\"Tokenizer {Fore.LIGHTBLUE_EX}{model_name}{Style.RESET_ALL} loaded successfully.\")\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "OSL_qjIGWdAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.2. Put an <font color=\"#22819F\"><b>eos_token</b></font> if the examples do not have one at the end"
      ],
      "metadata": {
        "id": "RFPqnjvOWQKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_pad_tokens_at_end(texts, tokenizer) -> list[str]:\n",
        "    processed_texts: list[str] = []\n",
        "\n",
        "    for text in texts:\n",
        "        if not text.endswith(tokenizer.eos_token):\n",
        "            text += tokenizer.eos_token\n",
        "\n",
        "        processed_texts.append(text)\n",
        "\n",
        "    return processed_texts"
      ],
      "metadata": {
        "id": "i0u8SqDd2Hb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.3. Change <font color=\"#22819F\">pad_token id</font> to reserved keyword"
      ],
      "metadata": {
        "id": "5BWYSusw2VwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def change_pad_tokens_ids_to_res_keyword(tokenized_text_id: list[int], tokenizer: Any) -> list[int]:\n",
        "    tokenized_texts_label_ids = []\n",
        "\n",
        "    for label_token_ids in tokenized_text_id:\n",
        "        processed_label_token_ids: list[str] = []\n",
        "\n",
        "        for token_id in label_token_ids:\n",
        "            if token_id == tokenizer.pad_token_id:\n",
        "                token_id = -100\n",
        "\n",
        "            processed_label_token_ids.append(token_id)\n",
        "\n",
        "        tokenized_texts_label_ids.append(processed_label_token_ids)\n",
        "\n",
        "    return tokenized_texts_label_ids"
      ],
      "metadata": {
        "id": "nntVOmHE2dpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.4. Tokenize"
      ],
      "metadata": {
        "id": "jRdbR9jK18zD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(examples: Dataset | DatasetDict | IterableDataset | IterableDatasetDict,\n",
        "             tokenizer: Any, max_length: int):\n",
        "    texts: list[str] = examples[\"text\"]\n",
        "    processed_texts: list[str] = add_pad_tokens_at_end(texts, tokenizer)\n",
        "\n",
        "    tokenized_texts = tokenizer(\n",
        "        processed_texts,\n",
        "        max_length = max_length,\n",
        "        truncation = True,\n",
        "        padding = \"max_length\",\n",
        "        return_tensors = \"pt\")\n",
        "\n",
        "    tokenized_text_input_id: list[int] = tokenized_texts[\"input_ids\"].clone()\n",
        "    tokenized_texts[\"labels\"] = change_pad_tokens_ids_to_res_keyword(tokenized_text_input_id, tokenizer)\n",
        "\n",
        "    return tokenized_texts"
      ],
      "metadata": {
        "id": "LgOOVs2p_-HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset_from_hugging_face(\n",
        "    dataset_name: str = dataset_name,\n",
        "    model_name: str = model_name,\n",
        "    max_samples: int = None,\n",
        "    max_length: int = 512\n",
        "    ) -> tuple[Dataset, Any]:\n",
        "    \"\"\"\n",
        "    Prepare dataset from HuggingFace.\n",
        "\n",
        "    Args:\n",
        "        dataset_name (str): Name of the dataset.\n",
        "        model_name (str): Name of the model.\n",
        "        max_samples (int): Maximum number of samples in the dataset to use.\n",
        "        max_length (int): Maximum length of the input.\n",
        "\n",
        "    Returns:\n",
        "        tuple[ds.Dataset, AutoTokenizer]: Prepared dataset and tokenizer.\n",
        "    \"\"\"\n",
        "    dataset = get_dataset(dataset_name, max_samples)\n",
        "\n",
        "    tokenizer = get_tokenizer(model_name)\n",
        "\n",
        "    tokenize_function_args_map: dict = {\n",
        "        \"tokenizer\": tokenizer,\n",
        "        \"max_length\": max_length\n",
        "        }\n",
        "\n",
        "    print(f\"{Fore.CYAN}Tokenization started{Style.RESET_ALL}\")\n",
        "    tokenized_dataset = dataset.map(tokenize,\n",
        "        fn_kwargs = tokenize_function_args_map,\n",
        "        remove_columns = dataset.column_names,\n",
        "        batched=True, desc = \"Tokenizing dataset\")\n",
        "    print(f\"{Fore.CYAN}Tokenization completed{Style.RESET_ALL}\")\n",
        "\n",
        "    return tokenized_dataset, tokenizer"
      ],
      "metadata": {
        "id": "XihRfRy-1vz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.5. Test Data preparation module"
      ],
      "metadata": {
        "id": "9YEr_r-kFddn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_dataset, tokenizer = prepare_dataset_from_hugging_face()"
      ],
      "metadata": {
        "id": "98zHD9hqCC-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_dataset"
      ],
      "metadata": {
        "id": "TWGbtFHgxsLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer"
      ],
      "metadata": {
        "id": "U1fLonSz8lfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. <font color = \"#EE0099\"><b>Lo</b></font>w <font color = \"#EE0099\"><b>R</b></font>ank <font color = \"#EE0099\"><b>A</b></font>daptation"
      ],
      "metadata": {
        "id": "UEU8237sFmPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2.1. Prepare <font color = \"#FF0077\">LoRA</font> Config\n",
        "<i>Note: This config class is a carrier of all values default and user assigned. It is not LoRA constructor in any way.</i>"
      ],
      "metadata": {
        "id": "QFIZKI41H0YN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ProductionLoRAConfig:\n",
        "    model_name: str = \"openai-community/gpt2\"\n",
        "    max_seq_length: int = 512\n",
        "    dtype: torch.dtype = torch.float16\n",
        "    lora_r: int = 16 # lora attention dimension, or rank of matrices: the minimum number of independent rows / columns in a matrix is rank.\n",
        "    lora_alpha: int = 16 # alpha parameter: factor which is multiplied to LoRA module matrices before it is added to original params. lower values makes LoRA params less significant.\n",
        "    lora_dropout: float = 0.05 # probability for dropping out the LoRA elements in low rank matrices to prevent overfitting. it is regularisation technique.\n",
        "    target_modules: list = None # modules are q_proj, K_proj, etc depending on the architecture\n",
        "    load_in_4bit: bool = True\n",
        "    bnb_4bit_compute_dtype: torch.dtype = torch.float16\n",
        "    bnb_4bit_quant_type: str = \"nf4\"\n",
        "    bnb_4bit_use_double_quant: bool = True\n",
        "    bnb_8bit_quant_type: str = \"nf4\"\n",
        "    bnb_8bit_use_double_quant: bool = True\n",
        "    bnb_8bit_compute_dtype: torch.dtype = torch.float16\n",
        "    max_steps: int = 2000 # risk of overfitting\n",
        "    per_device_train_batch_size: int = 2\n",
        "    gradient_accumulation_steps: int = 4\n",
        "    learning_rate: float = 2e-4\n",
        "    weight_decay: float = 0.001\n",
        "    warmup_steps: int = 100\n",
        "    warmup_ratio: int = 0.1\n",
        "    optimizer: str = \"adamw_8bith\"\n",
        "    save_steps: int = 250\n",
        "    save_total_limit: int = 3\n",
        "    eval_strategy: str = \"steps\"\n",
        "    eval_steps: int = 250\n",
        "    load_best_model_at_end: bool = True\n",
        "    metric_for_best_model: str = \"loss\"\n",
        "    greater_is_better: bool = False # for \"accuracy\" metric, make it true\n",
        "    gradient_checkpointing: bool = True # recommended for large models on a limited GPU memory. In Hugging Face platform, it is implemented as model.gradient_checkpointing_enabled().\n",
        "    dataloader_num_workers: int = 4\n",
        "    remove_unused_columns: bool = False\n",
        "    group_by_length: bool = True\n",
        "    ddp_find_unused_parameters: bool = False\n",
        "    logging_steps: int = 10\n",
        "    report_to: str = \"tensorboard\" # \"wandb\" or \"tensorboard\" or \"none\"\n",
        "    # task_type is ignored here as a param as we choose the type of LLM later on.\n",
        "\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.target_modules is None:\n",
        "            if \"openai-community/gpt2\" in self.model_name:\n",
        "                self.target_modules = [\"c_attn\", \"c_proj\"]\n",
        "            else:\n",
        "                self.target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "\n",
        "\n",
        "    def get_effective_batch_size(self) -> tuple[int, int]:\n",
        "        num_gpus: int = 1\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            num_gpus = torch.cuda.device_count()\n",
        "\n",
        "        effective_batch_size: int = self.per_device_train_batch_size * num_gpus * self.gradient_accumulation_steps\n",
        "\n",
        "        return effective_batch_size, num_gpus\n",
        "\n",
        "\n",
        "    def print_config(self):\n",
        "        print(\"LoRA Configuration\")\n",
        "\n",
        "        for field, value in self.__dict__.items():\n",
        "            if not field.startswith(\"_\"):\n",
        "                print(f\"{field}\\t: {Style.BRIGHT}{Fore.CYAN}{value}{Style.RESET_ALL}\")\n",
        "\n",
        "\n",
        "        effective_batch_size, num_gpus = self.get_effective_batch_size()\n",
        "\n",
        "        print(f\"\\n{'Effective Batch Size':30s}\\t: {Style.BRIGHT}{Fore.CYAN}{effective_batch_size}{Style.RESET_ALL} across {Style.BRIGHT}{Fore.RED}{num_gpus}{Style.RESET_ALL} GPU(s)\")"
      ],
      "metadata": {
        "id": "8BRy29t480c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2.2. Configure LoRA using the above carrier config class"
      ],
      "metadata": {
        "id": "GyGd-YsQFR1W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AqS6NATaGa4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2.3. Inject LoRA in the model using <font color = #5577FF>unsloth.FastLanguageModel.<b>get_peft_model(...)</b></font>"
      ],
      "metadata": {
        "id": "_qGKMtXTGbzS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "318GCWnGHSUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2.4. Train with unsloth"
      ],
      "metadata": {
        "id": "PtSlNSeEJlPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u-fZhTzHJrOJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}